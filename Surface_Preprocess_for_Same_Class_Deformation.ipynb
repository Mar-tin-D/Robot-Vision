{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Tc0NAncK_pY"
   },
   "source": [
    "\n",
    "\n",
    "This code is based on the work \"Dynamic Point Fields: Learning Surface Deformation by **Author**: [Sergey Prokudin](https://scholar.google.com/citations?user=xSywCzAAAAAJ).\n",
    "[[Project Page](https://sergeyprokudin.github.io/dpf/)]\n",
    "[[Paper](https://arxiv.org/abs/2304.02626)]\n",
    "[[Video](https://www.youtube.com/watch?v=i-9eAgS8HEA)]\n",
    "[[GitHub](https://github.com/sergeyprokudin/dpf)]\n",
    " and was adapted to the set task of \"Same Class Deformation\". Where a source model will be deformed to represent a target model, with AIAP as a limiting factor to keep the source shape to some degree.\n",
    "\n",
    "### Main steps:\n",
    "\n",
    "1. Download the mesh of choice (optional)\n",
    "2. Sample points from the surface to form initial point cloud;\n",
    "3. Optimise point locations and normals to fit the target mesh and its renderings, w.r.t. Chamfer distance, normal Chamfer distance and image-based normal loss;\n",
    "4. Render and save the final optimised cloud.\n",
    "5. Define the SIREN deformation network.\n",
    "6. Optimise deformation network based on the combination of losses: Chamfer distance, as-isometric-as-possible-regularisation (AIAP)\n",
    "7. Visualise the deformed source mesh\n",
    "8. Evaluate the deformed surface with its Chamfer distance\n",
    "\n",
    "\n",
    "### Notes\n",
    "For citing Dynamic Point Fields by Sergey Prokudin:\n",
    "```bibtex\n",
    "@article{prokudin2023dynamic,\n",
    "                  title={Dynamic Point Fields},\n",
    "                  author={Prokudin, Sergey and Ma, Qianli and Raafat, Maxime and Valentin, Julien and Tang, Siyu},\n",
    "                  journal={arXiv preprint arXiv:2304.02626},\n",
    "                  year={2023}\n",
    "                }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42102,
     "status": "ok",
     "timestamp": 1711045479162,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "79A8hHhlNeL-",
    "outputId": "84be055b-0e79-414d-8978-1f74b363185d"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies: pytorch3d, point-cloud-utils, trimesh\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
    "version_str=\"\".join([\n",
    "    f\"py3{sys.version_info.minor}_cu\",\n",
    "    torch.version.cuda.replace(\".\",\"\"),\n",
    "    f\"_pyt{pyt_version_str}\"\n",
    "])\n",
    "\"\"\"\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "!wget http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
    "\n",
    "with ZipFile('ModelNet10.zip', 'r') as ziped_file:\n",
    "    ziped_file.extractall()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Easiest pair\n",
    "\n",
    "source_file=\"/home/martin/Changed_Dynamic_Point_Field/ModelNet10/table/train/table_0025.off\"\n",
    "target_file=\"/home/martin/Changed_Dynamic_Point_Field/ModelNet10/table/train/table_0020.off\"\n",
    "test_file=\"/home/martin/Changed_Dynamic_Point_Field/ModelNet10/table/train/table_0031.off\"\n",
    "\n",
    "#another round table\n",
    "#\"ModelNet10\\table\\train\\table_0057.off\"\n",
    "\n",
    "#\"ModelNet10\\table\\train\\table_0057.off\"\n",
    "\n",
    "#\"/home/martin/Changed_Dynamic_Point_Field/ModelNet10/sofa/train/sofa_0045.off\"\n",
    "#\"/home/martin/Changed_Dynamic_Point_Field/ModelNet10/sofa/train/sofa_0062.off\"\n",
    "#\"/home/martin/Changed_Dynamic_Point_Field/ModelNet10/sofa/train/sofa_0042.off\"\n",
    "\n",
    "#\"/home/martin/Changed_Dynamic_Point_Field/ModelNet10/chair/train/chair_0003.off\"\n",
    "\n",
    "#\"/home/martin/Changed_Dynamic_Point_Field/Dino/mesh_clean1.ply\"\n",
    "#t\"/home/martin/Changed_Dynamic_Point_Field/Dino/mesh_clean2.ply\"\n",
    "#\"/home/martin/Changed_Dynamic_Point_Field/Dino/mesh_clean3.ply\"\n",
    "\n",
    "\n",
    "save_source='pcd_table0025.ply'\n",
    "save_target='pcd_table0020.ply'\n",
    "save_test='pcd_table0030.ply'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iX98JrKXY62B"
   },
   "outputs": [],
   "source": [
    "# @title Import libraries, define the necessary auxiliary functions (mesh normalisation, rendering, etc.)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  load OBJ files as pytorch3D meshes.\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "#from pytorch3d.io import _load_off_stream\n",
    "# import class \"meshes\" - see https://pytorch3d.readthedocs.io/en/latest/modules/structures.html\n",
    "from pytorch3d.structures import Meshes, Pointclouds\n",
    "#from pytorch3d.off_io import MeshOffFormat\n",
    "\n",
    "from pytorch3d.io import IO\n",
    "\n",
    "# to load an off mesh--------------------------------------------\n",
    "import trimesh\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# classes and modules from renderer - see https://pytorch3d.readthedocs.io/en/latest/modules/renderer/index.html\n",
    "from pytorch3d.renderer import (\n",
    "    Textures,\n",
    "    #TextureVertex was added by me\n",
    "    TexturesVertex,\n",
    "    look_at_view_transform,\n",
    "    # Field of View\n",
    "    FoVOrthographicCameras,\n",
    "    Materials,\n",
    "    #mapping from 3d scene to 2d image (not rendering , here is just the mapping)\n",
    "    RasterizationSettings,\n",
    "    BlendParams,\n",
    "    MeshRenderer,\n",
    "    #Rasterizes meshes into images\n",
    "    MeshRasterizer,\n",
    "    AmbientLights,\n",
    "    #color shaing ?\n",
    "    HardPhongShader,\n",
    ")\n",
    "\n",
    "# Constants for magic numbers\n",
    "DEVICE = 'cuda'\n",
    "DEFAULT_IMAGE_SIZE = 512\n",
    "NUM_IMAGES = 100\n",
    "DEFAULT_SCALE_VAL = 1.0\n",
    "DEFAULT_RANDOM_SEED = 13\n",
    "\n",
    "def normals_to_rgb(normals: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert mesh normals to RGB color representation.\n",
    "\n",
    "    Args:\n",
    "        normals (torch.Tensor): Mesh normals.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: RGB colors based on the normals.\n",
    "    \"\"\"\n",
    "    # value range is now [0,1] , befor it was [-1,1]\n",
    "    return torch.abs(normals * 0.5 + 0.5)\n",
    "\n",
    "def get_normals_as_textures(mesh: Meshes) -> Meshes:\n",
    "    \"\"\"\n",
    "    Create textures from mesh normals.\n",
    "\n",
    "    Args:\n",
    "        mesh (Meshes): Input mesh.\n",
    "\n",
    "    Returns:\n",
    "        Meshes: New mesh with normals as textures.\n",
    "    \"\"\"\n",
    "    #get normals as tensor from mesh object\n",
    "    normals = mesh.verts_normals_packed()\n",
    "\n",
    "    #change normals into range [0,1] , unsqueeze is needed for the \"Textures or TexturesVertex\" function\n",
    "    #\"Textures or TexturesVertex\" function creates with the \"normals_to_rgb\" colored textures for the meshes (see images above)\n",
    "    colors=normals_to_rgb(normals)\n",
    "    textures = TexturesVertex(colors.unsqueeze(0))\n",
    "\n",
    "    #takes the original (input) meseh vertices and faces and adds the color textures to the vertices and faces\n",
    "    return Meshes(mesh.verts_packed().unsqueeze(0), mesh.faces_packed().unsqueeze(0), textures)\n",
    "\n",
    "\n",
    "\n",
    "def normalize_verts(vertices: torch.Tensor, scale=None, center=None) -> tuple:\n",
    "    \"\"\"\n",
    "    Normalize vertex positions of a mesh.\n",
    "\n",
    "    Args:\n",
    "        vertices (torch.Tensor): Vertex positions.\n",
    "        scale (float, optional): Scaling factor. Defaults to None.\n",
    "        center (torch.Tensor, optional): Center of the mesh. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Normalized vertex positions, scaling factor, and center.\n",
    "    \"\"\"\n",
    "\n",
    "    # if center andscale known then centers the vertices around the origin and scales them\n",
    "    if scale is not None and center is not None:\n",
    "        vertices = vertices - center\n",
    "        vertices *= scale\n",
    "    else:\n",
    "      #calculate max and min of mesh vertices\n",
    "        v_max, _ = torch.max(vertices, dim=0)\n",
    "        v_min, _ = torch.min(vertices, dim=0)\n",
    "        #calculate center of mesh\n",
    "        center = (v_max + v_min) / 2.\n",
    "        #centering around origin\n",
    "        vertices = vertices - center\n",
    "        #calc max distance from vertex to origin (euklidean)\n",
    "        max_dist = torch.sqrt(torch.max(torch.sum(vertices**2, dim=-1)))\n",
    "        #scaling the max dist vertex to 1\n",
    "        scale = (1. / max_dist)\n",
    "        vertices *= scale\n",
    "\n",
    "        #return vertices , the scale and the center of the mesh\n",
    "    return vertices, scale, center\n",
    "\n",
    "def normalize_mesh(mesh: Meshes, scale=None, center=None) -> tuple:\n",
    "    \"\"\"\n",
    "    Normalize a mesh.\n",
    "\n",
    "    Args:\n",
    "        mesh (Meshes): Input mesh.\n",
    "        scale (float, optional): Scaling factor. Defaults to None.\n",
    "        center (torch.Tensor, optional): Center of the mesh. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Normalized mesh, scaling factor, and center.\n",
    "    \"\"\"\n",
    "    # use function from before\n",
    "    vertices, scale, center = normalize_verts(mesh.verts_packed(), scale, center)\n",
    "    #updates the mesh with the normalized vertices to create a new mesh\n",
    "    normalized_mesh = mesh.update_padded(vertices.unsqueeze(0))\n",
    "    return normalized_mesh, scale, center\n",
    "\n",
    "def create_mesh_renderer(cameras, image_size=DEFAULT_IMAGE_SIZE, device='cuda'):\n",
    "    \"\"\"\n",
    "    Create a mesh renderer.\n",
    "\n",
    "    Args:\n",
    "        cameras (FoVOrthographicCameras): Camera setup.\n",
    "        image_size (int, optional): Image size. Defaults to DEFAULT_IMAGE_SIZE.\n",
    "        device (str, optional): Device for computation. Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        MeshRenderer: Mesh renderer.\n",
    "    \"\"\"\n",
    "    materials = Materials(\n",
    "        device=device,\n",
    "        #Black and no shine\n",
    "        specular_color=[[0.0, 0.0, 0.0]],\n",
    "        shininess=0.0\n",
    "    )\n",
    "\n",
    "    raster_settings = RasterizationSettings(\n",
    "        image_size=image_size,\n",
    "        blur_radius=0.0,\n",
    "        faces_per_pixel=1,\n",
    "        bin_size=None,\n",
    "        cull_backfaces=True\n",
    "    )\n",
    "    # background color ?\n",
    "    blend_params = BlendParams(background_color=(0, 0, 0))\n",
    "\n",
    "    lights = AmbientLights(ambient_color=(1, 1, 1), device=device)\n",
    "\n",
    "    renderer = MeshRenderer(\n",
    "        rasterizer=MeshRasterizer(\n",
    "            cameras=cameras,\n",
    "            raster_settings=raster_settings\n",
    "        ),\n",
    "\n",
    "        shader=HardPhongShader(device=device,\n",
    "                               cameras=cameras,\n",
    "                               blend_params=blend_params,\n",
    "                               lights=lights,\n",
    "                               materials=materials)\n",
    "    )\n",
    "\n",
    "    return renderer\n",
    "\n",
    "def render_mesh(mesh, dist=1000, elev=0 , azim=0 ,image_size=DEFAULT_IMAGE_SIZE, radius=0.01, scale_val=0.02,device=DEVICE):\n",
    "    \"\"\"\n",
    "    Render a mesh.\n",
    "\n",
    "    Args:\n",
    "        mesh (Meshes): Input mesh.\n",
    "        dist (float, optional): Distance from the camera. Defaults to 1.\n",
    "        elev (float, optional): Elevation angle. Defaults to 0.\n",
    "        azim (float, optional): Azimuth angle. Defaults to 0.\n",
    "        image_size (int, optional): Image size. Defaults to DEFAULT_IMAGE_SIZE.\n",
    "        radius (float, optional): Radius. Defaults to 0.01.\n",
    "        scale_val (float, optional): Scaling value. Defaults to 1.0.\n",
    "        device (str, optional): Device to use for rendering. Defaults to DEVICE.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Rendered image.\n",
    "    \"\"\"\n",
    "    #get Rotation and Translation matrix for camera position at the set values\n",
    "    R, T = look_at_view_transform(dist=dist, elev=elev, azim=azim)\n",
    "    # move camera to the set position and scale\n",
    "    cam = FoVOrthographicCameras(R=R, T=T, scale_xyz=((scale_val, scale_val, scale_val),)).to(device)\n",
    "\n",
    "    renderer = create_mesh_renderer(cam, image_size=image_size)\n",
    "   \n",
    "    img = renderer(mesh, cameras=cam)[0]\n",
    "\n",
    "    return img\n",
    "\n",
    "def generate_render_data(mesh,\n",
    "                         n_images=NUM_IMAGES,\n",
    "                         scale_val=DEFAULT_SCALE_VAL,\n",
    "                         rand_seed=DEFAULT_RANDOM_SEED,\n",
    "                         image_size=DEFAULT_IMAGE_SIZE,\n",
    "                         device=DEVICE):\n",
    "    \"\"\"\n",
    "    Generate rendering data.\n",
    "\n",
    "    Args:\n",
    "        mesh (Meshes): Input mesh.\n",
    "        n_images (int, optional): Number of images to generate. Defaults to NUM_IMAGES.\n",
    "        scale_val (float, optional): Scaling value. Defaults to DEFAULT_SCALE_VAL.\n",
    "        rand_seed (int, optional): Random seed. Defaults to DEFAULT_RANDOM_SEED.\n",
    "        image_size (int, optional): Image size. Defaults to DEFAULT_IMAGE_SIZE.\n",
    "        device (str, optional): Device to use for rendering. Defaults to DEVICE.\n",
    "    Returns:\n",
    "        tuple: Azimuth angles, elevation angles, and rendered images.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(rand_seed)\n",
    "    ygts = []\n",
    "    azims, elevs = [], []\n",
    "    # get random angles\n",
    "    #tqdm is \"loading bar\"\n",
    "    for ix in tqdm(range(0, n_images)):\n",
    "        azims.append(np.random.choice(360))\n",
    "        elevs.append(np.random.choice(180))\n",
    "\n",
    "        #render the mesh with given angles\n",
    "        ygts.append(render_mesh(mesh.to(device),\n",
    "                                azim=azims[ix],\n",
    "                                elev=elevs[ix],\n",
    "                                scale_val=scale_val,\n",
    "                                image_size=image_size,\n",
    "                                ).detach().cpu()[:, :, 0:3])\n",
    "\n",
    "    #reset random seed ?\n",
    "    np.random.seed(None)\n",
    "\n",
    "    return azims, elevs, ygts\n",
    "\n",
    "    # with this the function for mesh preparations are complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36940,
     "status": "ok",
     "timestamp": 1711045527516,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "K3_3rYOgQW-7",
    "outputId": "a3aeda30-bb41-4272-e63a-9451b6464ce7"
   },
   "outputs": [],
   "source": [
    "# Load and normalise GT mesh\n",
    "\n",
    "\"\"\"\n",
    "# load mesh\n",
    "mesh = trimesh.load('table_0001.off')\n",
    "v,f = torch.from_numpy(mesh.vertices),torch.from_numpy(mesh.faces)\n",
    "mesh_gt = Meshes(verts=[v], faces=[f]).to(DEVICE)\n",
    "mesh_gt=mesh_gt.float()\n",
    "#GT_MESH_PATH = 'lego_gt.obj'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "mesh_gt_source = IO().load_mesh(source_file, device=DEVICE)\n",
    "mesh_gt_source = get_normals_as_textures(mesh_gt_source).to(DEVICE)\n",
    "mesh_gt_source_norm, vscale_source, vcenter_source = normalize_mesh(mesh_gt_source)\n",
    "\n",
    "mesh_gt_target = IO().load_mesh(target_file, device=DEVICE)\n",
    "mesh_gt_target = get_normals_as_textures(mesh_gt_target).to(DEVICE)\n",
    "mesh_gt_target_norm, vscale_target, vcenter_target = normalize_mesh(mesh_gt_target)\n",
    "\n",
    "mesh_gt_test = IO().load_mesh(test_file, device=DEVICE)\n",
    "mesh_gt_test = get_normals_as_textures(mesh_gt_test).to(DEVICE)\n",
    "mesh_gt_test_norm, vscale_test, vcenter_test = normalize_mesh(mesh_gt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253959,
     "status": "ok",
     "timestamp": 1711045781454,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "_XRbxtaxUoCx",
    "outputId": "d6735982-1011-4b03-c56d-9c74a1f2eb06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:06<00:00, 28.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 33.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:06<00:00, 33.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate renders of the GT mesh (used for training with image-space normal loss)\n",
    "\n",
    "N_TRAIN_IMAGES = 200\n",
    "#N_TRAIN_IMAGES = 100\n",
    "\n",
    "azims_train_source, elevs_train_source,  ygts_train_source = generate_render_data(mesh_gt_source_norm,\n",
    "                                                        n_images=N_TRAIN_IMAGES,\n",
    "                                                        scale_val=1.0,\n",
    "                                                        rand_seed=19,\n",
    "                                                        image_size=DEFAULT_IMAGE_SIZE)\n",
    "\n",
    "azims_train_target, elevs_train_target,  ygts_train_target = generate_render_data(mesh_gt_target_norm,\n",
    "                                                        n_images=N_TRAIN_IMAGES,\n",
    "                                                        scale_val=1.0,\n",
    "                                                        rand_seed=19,\n",
    "                                                        image_size=DEFAULT_IMAGE_SIZE)\n",
    "\n",
    "azims_train_test, elevs_train_test,  ygts_train_test = generate_render_data(mesh_gt_test_norm,\n",
    "                                                        n_images=N_TRAIN_IMAGES,\n",
    "                                                        scale_val=1.0,\n",
    "                                                        rand_seed=19,\n",
    "                                                        image_size=DEFAULT_IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1711045781455,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "tDqzrSMObE9F",
    "outputId": "a1c57015-7a72-4c4c-c0a4-c48061625c22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\n\\n# visualised GT mesh\\nplt.figure()\\nplt.title(\"GT mesh source\")\\nplt.imshow(ygts_train_source[np.random.choice(N_TRAIN_IMAGES)])\\n\\nplt.figure()\\nplt.title(\"GT mesh target\")\\nplt.imshow(ygts_train_target[np.random.choice(N_TRAIN_IMAGES)])\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualised GT mesh\n",
    "plt.figure()\n",
    "plt.title(\"GT mesh source\")\n",
    "plt.imshow(ygts_train_source[np.random.choice(N_TRAIN_IMAGES)])\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"GT mesh target\")\n",
    "plt.imshow(ygts_train_target[np.random.choice(N_TRAIN_IMAGES)])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pNJj1Fi4oraS"
   },
   "outputs": [],
   "source": [
    "# @title Define functions for point rendering and optimisation\n",
    "\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.renderer import (\n",
    "    PointsRasterizationSettings,\n",
    "    PointsRasterizer,\n",
    "    PointsRenderer,\n",
    "    AlphaCompositor\n",
    ")\n",
    "from pytorch3d.loss import chamfer_distance\n",
    "\n",
    "POINTS_PER_PIXEL = 50\n",
    "\n",
    "def get_point_renderer(image_size, radius=0.05, points_per_pixel=50):\n",
    "\n",
    "  raster_settings = PointsRasterizationSettings(\n",
    "      image_size=image_size,\n",
    "      radius = radius,\n",
    "      points_per_pixel = points_per_pixel\n",
    "      )\n",
    "\n",
    "  rasterizer = PointsRasterizer(cameras=FoVOrthographicCameras(),\n",
    "                                raster_settings=raster_settings)\n",
    "  renderer = PointsRenderer(\n",
    "      rasterizer=rasterizer,\n",
    "      compositor=AlphaCompositor(background_color=(1, 1, 1))\n",
    "\n",
    "  )\n",
    "\n",
    "  return renderer\n",
    "\n",
    "def render_points(x, xf, dist=1, elev=0, azim=0, image_size=512,\n",
    "                  radius=0.01, points_per_pixel=50, scale_val=1.0,\n",
    "                  device=DEVICE):\n",
    "\n",
    "  x = x.to(device)\n",
    "  xf = xf.to(device)\n",
    "  renderer = get_point_renderer(image_size=image_size, radius=radius, points_per_pixel=points_per_pixel)\n",
    "  R, T = look_at_view_transform(dist=dist, elev=elev, azim=azim)\n",
    "  cam = FoVOrthographicCameras(R=R, T=T, scale_xyz=((scale_val, scale_val, scale_val),)).to(device)\n",
    "\n",
    "  pcl = Pointclouds(points=x.unsqueeze(0), features=xf.unsqueeze(0)).to(device)\n",
    "\n",
    "  img = renderer(pcl, cameras=cam)[0]\n",
    "\n",
    "  return img\n",
    "\n",
    "\n",
    "# @title Pymeshlab normal estimation\n",
    "\n",
    "# https://github.com/cnr-isti-vclab/PyMeshLab/blob/main/docs/filter_list.rst\n",
    "# https://pymeshlab.readthedocs.io/en/0.2/tutorials/get_mesh_values.html\n",
    "\n",
    "import pymeshlab\n",
    "\n",
    "def get_normals_pymeshlab(x,  k=10, smoothiter=0):\n",
    "\n",
    "  tmp_mesh_path = 'tmp.obj'\n",
    "  pcu.save_mesh_v(tmp_mesh_path, x)\n",
    "\n",
    "  ms = pymeshlab.MeshSet()\n",
    "  ms.load_new_mesh(tmp_mesh_path)\n",
    "  ms.compute_normal_for_point_clouds(k=k, smoothiter=smoothiter)\n",
    "  #ms.save_current_mesh(trg_path, save_vertex_normal=True)\n",
    "  #ms.generate_surface_reconstruction_screened_poisson(depth=8)\n",
    "  m = ms.current_mesh()\n",
    "\n",
    "  x = m.vertex_matrix()\n",
    "  n = m.vertex_normal_matrix()\n",
    "\n",
    "  return x, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fQW_uI_jqTvg"
   },
   "outputs": [],
   "source": [
    "# Sample initial points and corresponding normals\n",
    "\n",
    "import point_cloud_utils as pcu\n",
    "\n",
    "# uncomment the line for desired number of points and corresponding radius\n",
    "# NUM_POINTS, POINT_RADIUS = 10**4, 0.025\n",
    "# NUM_POINTS, POINT_RADIUS = 10**5, 0.009\n",
    "NUM_POINTS, POINT_RADIUS = 3*10**5, 0.007\n",
    "# NUM_POINTS, POINT_RADIUS = 10**6, 0.004\n",
    "\n",
    "#return points and corresponding normals\n",
    "v0_source, n0_source = sample_points_from_meshes(mesh_gt_source.to(DEVICE),\n",
    "                                   return_normals=True,\n",
    "                                   num_samples=NUM_POINTS)\n",
    "# and saved as paramters for optimization, grad is for gradient descent for autograd?\n",
    "v0_source = torch.nn.Parameter(v0_source[0].clone().to(DEVICE), requires_grad=True)\n",
    "n0_source = torch.nn.Parameter(n0_source[0].clone().to(DEVICE), requires_grad=True)\n",
    "\n",
    "#save as \"ply\" file\n",
    "# save the initial cloud for evaluation at the end\n",
    "pcu.save_mesh_vn('init_source.ply', v0_source.detach().cpu().numpy(), n0_source.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "v0_target, n0_target = sample_points_from_meshes(mesh_gt_target.to(DEVICE),\n",
    "                                   return_normals=True,\n",
    "                                   num_samples=NUM_POINTS)\n",
    "\n",
    "# and saved as paramters for optimization\n",
    "v0_target = torch.nn.Parameter(v0_target[0].clone().to(DEVICE), requires_grad=True)\n",
    "n0_target = torch.nn.Parameter(n0_target[0].clone().to(DEVICE), requires_grad=True)\n",
    "\n",
    "#save as \"ply\" file\n",
    "pcu.save_mesh_vn('init_target.ply', v0_target.detach().cpu().numpy(), n0_target.detach().cpu().numpy())\n",
    "\n",
    "v0_test, n0_test = sample_points_from_meshes(mesh_gt_test.to(DEVICE),\n",
    "                                   return_normals=True,\n",
    "                                   num_samples=NUM_POINTS)\n",
    "\n",
    "# and saved as paramters for optimization\n",
    "v0_test = torch.nn.Parameter(v0_test[0].clone().to(DEVICE), requires_grad=True)\n",
    "n0_test = torch.nn.Parameter(n0_test[0].clone().to(DEVICE), requires_grad=True)\n",
    "\n",
    "#save as \"ply\" file\n",
    "pcu.save_mesh_vn('init_test.ply', v0_test.detach().cpu().numpy(), n0_test.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 922
    },
    "executionInfo": {
     "elapsed": 2110,
     "status": "ok",
     "timestamp": 1711045783552,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "Ez9ssJjAzYYZ",
    "outputId": "80f34c55-4991-409d-f42e-6befe3c15674"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure()\\nplt.title(\"Initial cloud (non-optimized) target\")\\nplt.imshow(ypred_init_target)\\n\\nplt.figure()\\nplt.title(\"Ground truth mesh target\")\\nplt.imshow(ygt_demo_target)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# render initial, non-optimised cloud\n",
    "\n",
    "#select random iamge from train data set\n",
    "DEMO_FRAME_IX = np.random.choice(N_TRAIN_IMAGES)\n",
    "\n",
    "#normalize the points of the pointcloud v0 around the origin, like before with mesehs\n",
    "v0_norm_source, _, _ = normalize_verts(v0_source, vscale_source, vcenter_source)\n",
    "\n",
    "ypred_init_source = render_points(v0_norm_source.to(DEVICE),\n",
    "                      normals_to_rgb(n0_source.to(DEVICE)),\n",
    "                      elev=elevs_train_source[DEMO_FRAME_IX],\n",
    "                      azim=azims_train_source[DEMO_FRAME_IX],\n",
    "                      image_size=DEFAULT_IMAGE_SIZE,\n",
    "                      points_per_pixel=POINTS_PER_PIXEL,\n",
    "                      radius=POINT_RADIUS).detach().cpu().numpy()\n",
    "\n",
    "#print a demo of the point cloud and the corresponding ground truth mesh\n",
    "ygt_demo_source = ygts_train_source[DEMO_FRAME_IX].detach().cpu().numpy()\n",
    "\"\"\"\n",
    "plt.figure()\n",
    "plt.title(\"Initial cloud (non-optimized) source\")\n",
    "plt.imshow(ypred_init_source)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Ground truth mesh source\")\n",
    "plt.imshow(ygt_demo_source)\n",
    "\"\"\"\n",
    "\n",
    "#select random iamge from train data set\n",
    "DEMO_FRAME_IX = np.random.choice(N_TRAIN_IMAGES)\n",
    "\n",
    "#normalize the points(vectrices) of the pointcloud v0 around the origin, like before with mesehs\n",
    "v0_norm_target, _, _ = normalize_verts(v0_target, vscale_target, vcenter_target)\n",
    "\n",
    "ypred_init_target = render_points(v0_norm_target.to(DEVICE),\n",
    "                      normals_to_rgb(n0_target.to(DEVICE)),\n",
    "                      elev=elevs_train_target[DEMO_FRAME_IX],\n",
    "                      azim=azims_train_target[DEMO_FRAME_IX],\n",
    "                      image_size=DEFAULT_IMAGE_SIZE,\n",
    "                      points_per_pixel=POINTS_PER_PIXEL,\n",
    "                      radius=POINT_RADIUS).detach().cpu().numpy()\n",
    "\n",
    "#print a demo of the point cloud and the corresponding ground truth mesh\n",
    "ygt_demo_test = ygts_train_test[DEMO_FRAME_IX].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "#select random iamge from train data set\n",
    "DEMO_FRAME_IX = np.random.choice(N_TRAIN_IMAGES)\n",
    "\n",
    "#normalize the points(vectrices) of the pointcloud v0 around the origin, like before with mesehs\n",
    "v0_norm_test, _, _ = normalize_verts(v0_test, vscale_test, vcenter_test)\n",
    "\n",
    "ypred_init_test = render_points(v0_norm_test.to(DEVICE),\n",
    "                      normals_to_rgb(n0_test.to(DEVICE)),\n",
    "                      elev=elevs_train_test[DEMO_FRAME_IX],\n",
    "                      azim=azims_train_test[DEMO_FRAME_IX],\n",
    "                      image_size=DEFAULT_IMAGE_SIZE,\n",
    "                      points_per_pixel=POINTS_PER_PIXEL,\n",
    "                      radius=POINT_RADIUS).detach().cpu().numpy()\n",
    "\n",
    "#print a demo of the point cloud and the corresponding ground truth mesh\n",
    "ygt_demo_test = ygts_train_test[DEMO_FRAME_IX].detach().cpu().numpy()\n",
    "\n",
    "\"\"\"\n",
    "plt.figure()\n",
    "plt.title(\"Initial cloud (non-optimized) target\")\n",
    "plt.imshow(ypred_init_target)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Ground truth mesh target\")\n",
    "plt.imshow(ygt_demo_target)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 944436,
     "status": "ok",
     "timestamp": 1711046727972,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "DDUV4bUSZ8oa",
    "outputId": "58ed7aeb-820c-4dc3-caa1-a8c07f2ed87c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████▎                                                               | 1/4 [00:08<00:25,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 68.252916, cosine normal loss: 0.025553,  L2 image normal loss: 7.183423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████▌                                          | 2/4 [00:16<00:16,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 67.961147, cosine normal loss: 0.025488,  L2 image normal loss: 7.157850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████▊                     | 3/4 [00:24<00:08,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 67.773327, cosine normal loss: 0.026230,  L2 image normal loss: 7.108978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:33<00:00,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 67.560881, cosine normal loss: 0.027013,  L2 image normal loss: 6.966903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Run point cloud optimisation\n",
    "#NUM_EPOCHS = 4\n",
    "#NUM_GT_EVAL_POINTS = 10**6\n",
    "#N_ITERS_PER_EPOCH = 20\n",
    "\n",
    "#NUM_EPOCHS = 1\n",
    "#NUM_GT_EVAL_POINTS = 10**2\n",
    "#N_ITERS_PER_EPOCH = 20\n",
    "\n",
    "NUM_EPOCHS = 4\n",
    "NUM_GT_EVAL_POINTS = 10**5\n",
    "N_ITERS_PER_EPOCH = 20\n",
    "\n",
    "\n",
    "#vs_source = []\n",
    "#ns_source = []\n",
    "\n",
    "#mean squared error loss function\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "#Adam = Adaptive Moment Estimation (AdaGrad and RMSProp)\n",
    "#optimazation used to train deep learning models\n",
    "optv = torch.optim.Adam([v0_source], lr=1.0e-4)\n",
    "optn = torch.optim.Adam([n0_source], lr=1.0e-2)\n",
    "\n",
    "#scheduler for vertex , when a there is only little change in loss then the learning rate is reduced\n",
    "schedv = torch.optim.lr_scheduler.ReduceLROnPlateau(optv, patience=1)\n",
    "\n",
    "\n",
    "#same for normals\n",
    "schedn = torch.optim.lr_scheduler.ReduceLROnPlateau(optn, patience=1)\n",
    "\n",
    "\n",
    "#total loss\n",
    "tloss = 0\n",
    "n_r = 0\n",
    "\n",
    "#loading bar\n",
    "for i in tqdm(range(0, NUM_EPOCHS)):\n",
    "\n",
    "  # random set of training images is selected ( wiht no replace)\n",
    "  epoch_fids = np.random.choice(len(azims_train_source), N_ITERS_PER_EPOCH, replace=False)\n",
    "\n",
    "\n",
    "  chamfer_loss_total = 0\n",
    "  l2_norm_total = 0\n",
    "  cosine_normal_loss_total = 0\n",
    "  n_r = 0\n",
    "# work through the selected set\n",
    "  for ix in epoch_fids:\n",
    "\n",
    "#sample points from mesh\n",
    "    vgt_source, vgtn_source = sample_points_from_meshes(mesh_gt_source.to(DEVICE),\n",
    "                        return_normals=True,\n",
    "                        num_samples=NUM_GT_EVAL_POINTS)\n",
    "\n",
    "#normalize the points\n",
    "    v0_norm_source, _, _ = normalize_verts(v0_source, vscale_source, vcenter_source)\n",
    "    ypred_source = render_points(v0_norm_source.to(DEVICE),\n",
    "                      normals_to_rgb(n0_source.to(DEVICE)),\n",
    "                      elev=elevs_train_source[ix],\n",
    "                      azim=azims_train_source[ix],\n",
    "                      image_size=DEFAULT_IMAGE_SIZE,\n",
    "                      points_per_pixel=POINTS_PER_PIXEL,\n",
    "                      radius=POINT_RADIUS)\n",
    "\n",
    "#mse error between the ground truth mesh and the render sampled points\n",
    "    l2_norm = 10*mse_loss(ygts_train_source[ix].to(DEVICE), ypred_source)\n",
    "#chamfer loss - the sum of the min distance between each point of cloud A to B and cloud B to A\n",
    "#cosine loss - how similar are the normals of these closest points\n",
    "    chamfer_loss, cosine_normal_loss = chamfer_distance(vgt_source, v0_source.unsqueeze(0), x_normals=vgtn_source, y_normals=n0_source.unsqueeze(0), norm=2)\n",
    "\n",
    "    chamfer_loss, normal_loss = 10**4*chamfer_loss, cosine_normal_loss\n",
    "    loss = chamfer_loss + cosine_normal_loss + 10*l2_norm\n",
    "\n",
    "    chamfer_loss_total += float(chamfer_loss)\n",
    "    cosine_normal_loss_total += float(cosine_normal_loss)\n",
    "    l2_norm_total += float(l2_norm)\n",
    "    n_r += 1\n",
    "    optv.zero_grad()\n",
    "    optn.zero_grad()\n",
    "    loss.backward()\n",
    "    optv.step()\n",
    "    optn.step()\n",
    "\n",
    "\n",
    "  chamfer_loss_total /= n_r\n",
    "  l2_norm_total /= n_r\n",
    "  cosine_normal_loss_total /= n_r\n",
    "  print(\"Chamfer loss: %f, cosine normal loss: %f,  L2 image normal loss: %f\" % (chamfer_loss_total, cosine_normal_loss_total, l2_norm_total))\n",
    "  total_loss = chamfer_loss_total + l2_norm_total\n",
    "  schedv.step(float(total_loss))\n",
    "  schedn.step(float(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 944436,
     "status": "ok",
     "timestamp": 1711046727972,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "DDUV4bUSZ8oa",
    "outputId": "58ed7aeb-820c-4dc3-caa1-a8c07f2ed87c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████▎                                                               | 1/4 [00:08<00:24,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 411.876912, cosine normal loss: 0.026492,  L2 image normal loss: 7.968194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████▌                                          | 2/4 [00:16<00:16,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 411.222572, cosine normal loss: 0.025679,  L2 image normal loss: 7.943242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████▊                     | 3/4 [00:24<00:08,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 410.630023, cosine normal loss: 0.025696,  L2 image normal loss: 7.593109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:33<00:00,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 410.280080, cosine normal loss: 0.025926,  L2 image normal loss: 7.527642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## @title Run point cloud optimisation\n",
    "\n",
    "\n",
    "#mean squared error loss function\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "#Adam = Adaptive Moment Estimation (AdaGrad and RMSProp)\n",
    "#optimazation used to train deep learning models\n",
    "optv = torch.optim.Adam([v0_target], lr=1.0e-4)\n",
    "optn = torch.optim.Adam([n0_target], lr=1.0e-2)\n",
    "\n",
    "#scheduler for vertex , when a there is only little change in loss then the learning rate is reduced\n",
    "schedv = torch.optim.lr_scheduler.ReduceLROnPlateau(optv, patience=1)\n",
    "\n",
    "#same for normals\n",
    "schedn = torch.optim.lr_scheduler.ReduceLROnPlateau(optn, patience=1)\n",
    "\n",
    "tloss = 0\n",
    "n_r = 0\n",
    "\n",
    "#loading bar\n",
    "for i in tqdm(range(0, NUM_EPOCHS)):\n",
    "\n",
    "  # random set of training images is selected ( wiht no replace)\n",
    "  epoch_fids = np.random.choice(len(azims_train_target), N_ITERS_PER_EPOCH, replace=False)\n",
    "\n",
    "\n",
    "  chamfer_loss_total = 0\n",
    "  l2_norm_total = 0\n",
    "  cosine_normal_loss_total = 0\n",
    "  n_r = 0\n",
    "# work through the selected set\n",
    "  for ix in epoch_fids:\n",
    "\n",
    "    #sample points from mesh\n",
    "    vgt_target, vgtn_target = sample_points_from_meshes(mesh_gt_target.to(DEVICE),\n",
    "                        return_normals=True,\n",
    "                        num_samples=NUM_GT_EVAL_POINTS)\n",
    "\n",
    "    #normalize the points\n",
    "    v0_norm_target, _, _ = normalize_verts(v0_target, vscale_target, vcenter_target)\n",
    "    ypred_target = render_points(v0_norm_target.to(DEVICE),\n",
    "                      normals_to_rgb(n0_target.to(DEVICE)),\n",
    "                      elev=elevs_train_target[ix],\n",
    "                      azim=azims_train_target[ix],\n",
    "                      image_size=DEFAULT_IMAGE_SIZE,\n",
    "                      points_per_pixel=POINTS_PER_PIXEL,\n",
    "                      radius=POINT_RADIUS)\n",
    "\n",
    "    #mse error between the ground truth mesh and the render sampled points (why times 10???)\n",
    "    l2_norm = 10*mse_loss(ygts_train_target[ix].to(DEVICE), ypred_target)\n",
    "    #chamfer loss - the sum of the min distance between each point of cloud A to B and cloud B to A\n",
    "    #cosine loss - how similar are the normals of these closes points ?\n",
    "    chamfer_loss, cosine_normal_loss = chamfer_distance(vgt_target, v0_target.unsqueeze(0), x_normals=vgtn_target, y_normals=n0_target.unsqueeze(0), norm=2)\n",
    "\n",
    "    chamfer_loss, normal_loss = 10**4*chamfer_loss, cosine_normal_loss\n",
    "    loss = chamfer_loss + cosine_normal_loss + 10*l2_norm\n",
    "\n",
    "    chamfer_loss_total += float(chamfer_loss)\n",
    "    cosine_normal_loss_total += float(cosine_normal_loss)\n",
    "    l2_norm_total += float(l2_norm)\n",
    "    n_r += 1\n",
    "    optv.zero_grad()\n",
    "    optn.zero_grad()\n",
    "    loss.backward()\n",
    "    optv.step()\n",
    "    optn.step()\n",
    "\n",
    "    # n_r number of iterations ????\n",
    "  chamfer_loss_total /= n_r\n",
    "  l2_norm_total /= n_r\n",
    "  cosine_normal_loss_total /= n_r\n",
    "  print(\"Chamfer loss: %f, cosine normal loss: %f,  L2 image normal loss: %f\" % (chamfer_loss_total, cosine_normal_loss_total, l2_norm_total))\n",
    "  total_loss = chamfer_loss_total + l2_norm_total\n",
    "  schedv.step(float(total_loss))\n",
    "  schedn.step(float(total_loss))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 944436,
     "status": "ok",
     "timestamp": 1711046727972,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "DDUV4bUSZ8oa",
    "outputId": "58ed7aeb-820c-4dc3-caa1-a8c07f2ed87c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████▎                                                               | 1/4 [00:08<00:24,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 334.796335, cosine normal loss: 0.045757,  L2 image normal loss: 7.862214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████▌                                          | 2/4 [00:16<00:16,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 333.818434, cosine normal loss: 0.043866,  L2 image normal loss: 7.691204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████▊                     | 3/4 [00:24<00:08,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 333.438287, cosine normal loss: 0.043066,  L2 image normal loss: 8.017503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:33<00:00,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamfer loss: 332.972731, cosine normal loss: 0.043050,  L2 image normal loss: 7.609788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## @title Run point cloud optimisation\n",
    "\n",
    "\n",
    "#mean squared error loss function\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "#Adam = Adaptive Moment Estimation (AdaGrad and RMSProp)\n",
    "#optimazation used to train deep learning models\n",
    "optv = torch.optim.Adam([v0_test], lr=1.0e-4)\n",
    "optn = torch.optim.Adam([n0_test], lr=1.0e-2)\n",
    "\n",
    "#scheduler for vertex , when a there is only little change in loss then the learning rate is reduced\n",
    "schedv = torch.optim.lr_scheduler.ReduceLROnPlateau(optv, patience=1)\n",
    "\n",
    "\n",
    "#same for normals\n",
    "schedn = torch.optim.lr_scheduler.ReduceLROnPlateau(optn, patience=1)\n",
    "\n",
    "\n",
    "tloss = 0\n",
    "n_r = 0\n",
    "\n",
    "#loading bar\n",
    "for i in tqdm(range(0, NUM_EPOCHS)):\n",
    "\n",
    "  # random set of training images is selected ( wiht no replace)\n",
    "  epoch_fids = np.random.choice(len(azims_train_test), N_ITERS_PER_EPOCH, replace=False)\n",
    "\n",
    "\n",
    "  chamfer_loss_total = 0\n",
    "  l2_norm_total = 0\n",
    "  cosine_normal_loss_total = 0\n",
    "  n_r = 0\n",
    "# work through the selected set\n",
    "  for ix in epoch_fids:\n",
    "\n",
    "#sample points from mesh\n",
    "    vgt_test, vgtn_test = sample_points_from_meshes(mesh_gt_test.to(DEVICE),\n",
    "                        return_normals=True,\n",
    "                        num_samples=NUM_GT_EVAL_POINTS)\n",
    "\n",
    "#normalize the points\n",
    "    v0_norm_test, _, _ = normalize_verts(v0_test, vscale_test, vcenter_test)\n",
    "    ypred_test = render_points(v0_norm_test.to(DEVICE),\n",
    "                      normals_to_rgb(n0_test.to(DEVICE)),\n",
    "                      elev=elevs_train_test[ix],\n",
    "                      azim=azims_train_test[ix],\n",
    "                      image_size=DEFAULT_IMAGE_SIZE,\n",
    "                      points_per_pixel=POINTS_PER_PIXEL,\n",
    "                      radius=POINT_RADIUS)\n",
    "\n",
    "    #mse error between the ground truth mesh and the render sampled points (why times 10???)\n",
    "    l2_norm = 10*mse_loss(ygts_train_test[ix].to(DEVICE), ypred_test)\n",
    "    #chamfer loss - the sum of the min distance between each point of cloud A to B and cloud B to A\n",
    "    #cosine loss - how similar are the normals of these closes points ?\n",
    "    chamfer_loss, cosine_normal_loss = chamfer_distance(vgt_test, v0_test.unsqueeze(0), x_normals=vgtn_test, y_normals=n0_test.unsqueeze(0), norm=2)\n",
    "      \n",
    "    chamfer_loss, normal_loss = 10**4*chamfer_loss, cosine_normal_loss\n",
    "    loss = chamfer_loss + cosine_normal_loss + 10*l2_norm\n",
    "\n",
    "    chamfer_loss_total += float(chamfer_loss)\n",
    "    cosine_normal_loss_total += float(cosine_normal_loss)\n",
    "    l2_norm_total += float(l2_norm)\n",
    "    n_r += 1\n",
    "    optv.zero_grad()\n",
    "    optn.zero_grad()\n",
    "    loss.backward()\n",
    "    optv.step()\n",
    "    optn.step()\n",
    "\n",
    "\n",
    "  chamfer_loss_total /= n_r\n",
    "  l2_norm_total /= n_r\n",
    "  cosine_normal_loss_total /= n_r\n",
    "  print(\"Chamfer loss: %f, cosine normal loss: %f,  L2 image normal loss: %f\" % (chamfer_loss_total, cosine_normal_loss_total, l2_norm_total))\n",
    "  total_loss = chamfer_loss_total + l2_norm_total\n",
    "  schedv.step(float(total_loss))\n",
    "  schedn.step(float(total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3081,
     "status": "ok",
     "timestamp": 1711046731009,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "lLUF4w1Ut2BA",
    "outputId": "63bb0ca5-4875-4202-9554-9aa1483cdde7"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid number of dimensions for argument 'v_positions'. Expected 2 but got 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Render the optimised cloud\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ypred_source \u001b[38;5;241m=\u001b[39m render_points(v0_norm_source\u001b[38;5;241m.\u001b[39mto(DEVICE),\n\u001b[1;32m      4\u001b[0m                       normals_to_rgb(n0_source\u001b[38;5;241m.\u001b[39mto(DEVICE)),\n\u001b[1;32m      5\u001b[0m                       elev\u001b[38;5;241m=\u001b[39melevs_train_source[DEMO_FRAME_IX],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m                       points_per_pixel\u001b[38;5;241m=\u001b[39mPOINTS_PER_PIXEL,\n\u001b[1;32m      9\u001b[0m                       radius\u001b[38;5;241m=\u001b[39mPOINT_RADIUS)\n\u001b[0;32m---> 11\u001b[0m ypred_source, npred_source \u001b[38;5;241m=\u001b[39m \u001b[43mget_normals_pymeshlab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mypred_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m     14\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial cloud (non-optimized)_source\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 60\u001b[0m, in \u001b[0;36mget_normals_pymeshlab\u001b[0;34m(x, k, smoothiter)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_normals_pymeshlab\u001b[39m(x,  k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, smoothiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     59\u001b[0m   tmp_mesh_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp.obj\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 60\u001b[0m   \u001b[43mpcu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_mesh_v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_mesh_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m   ms \u001b[38;5;241m=\u001b[39m pymeshlab\u001b[38;5;241m.\u001b[39mMeshSet()\n\u001b[1;32m     63\u001b[0m   ms\u001b[38;5;241m.\u001b[39mload_new_mesh(tmp_mesh_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/point_cloud_utils/_mesh_io.py:431\u001b[0m, in \u001b[0;36msave_mesh_v\u001b[0;34m(filename, v, dtype)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_mesh_v\u001b[39m(filename, v, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32):\n\u001b[1;32m    422\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    Save a point cloud consisting only of vertex positions.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;124;03m    Point Cloud Utils currently supports PLY, OBJ, STL, OFF, VRML 2.0, X3D, COLLADA, 3DS.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m      dtype : The floating point written to the file (`np.float32` or `np.float64`)\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     \u001b[43msave_triangle_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/point_cloud_utils/_mesh_io.py:418\u001b[0m, in \u001b[0;36msave_triangle_mesh\u001b[0;34m(filename, v, f, vn, vt, vc, vq, vr, vti, vflags, fn, fc, fq, fflags, wc, wn, wt, wti, textures, normal_maps, dtype)\u001b[0m\n\u001b[1;32m    415\u001b[0m mesh\u001b[38;5;241m.\u001b[39mtextures \u001b[38;5;241m=\u001b[39m textures\n\u001b[1;32m    416\u001b[0m mesh\u001b[38;5;241m.\u001b[39mnormal_maps \u001b[38;5;241m=\u001b[39m normal_maps\n\u001b[0;32m--> 418\u001b[0m \u001b[43mmesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/point_cloud_utils/_mesh_io.py:285\u001b[0m, in \u001b[0;36mTriangleMesh.save\u001b[0;34m(self, filename, dtype)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wcolors\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m wcolors\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid values for wedge colors, must be between 0 and 1 (inclusive)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 285\u001b[0m \u001b[43msave_mesh_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexcoords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvcolors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mradius\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtex_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \n\u001b[1;32m    295\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfcolors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwcolors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwedge_normals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwedge_texcoords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwedge_tex_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvertex_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_attributes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mascontiguousarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_attributes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvertex_data\u001b[38;5;241m.\u001b[39m_set_empty_to_none()\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_data\u001b[38;5;241m.\u001b[39m_set_empty_to_none()\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid number of dimensions for argument 'v_positions'. Expected 2 but got 3."
     ]
    }
   ],
   "source": [
    "# Render the optimised cloud\n",
    "\n",
    "ypred_source = render_points(v0_norm_source.to(DEVICE),\n",
    "                      normals_to_rgb(n0_source.to(DEVICE)),\n",
    "                      elev=elevs_train_source[DEMO_FRAME_IX],\n",
    "                      azim=azims_train_source[DEMO_FRAME_IX],\n",
    "                      image_size=DEFAULT_IMAGE_SIZE,\n",
    "                      points_per_pixel=POINTS_PER_PIXEL,\n",
    "                      radius=POINT_RADIUS)\n",
    "\n",
    "ypred_source, npred_source = get_normals_pymeshlab(ypred_source.detach().cpu().numpy(), k=20, smoothiter=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Initial cloud (non-optimized)_source\")\n",
    "plt.imshow(ypred_init_source)\n",
    "\n",
    "#ypred_source_color=normalize_verts_color(ypred_source)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Optimised point cloud_source\")\n",
    "plt.imshow(ypred_source.detach().cpu().numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"GT mesh_source\")\n",
    "plt.imshow(ygts_train_source[DEMO_FRAME_IX][:, :, 0:3].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3081,
     "status": "ok",
     "timestamp": 1711046731009,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "lLUF4w1Ut2BA",
    "outputId": "63bb0ca5-4875-4202-9554-9aa1483cdde7"
   },
   "outputs": [],
   "source": [
    "## Render the optimised cloud\n",
    "\n",
    "ypred_target = render_points(v0_norm_target.to(DEVICE),\n",
    "                      normals_to_rgb(n0_target.to(DEVICE)),\n",
    "                      elev=elevs_train_target[DEMO_FRAME_IX],\n",
    "                      azim=azims_train_target[DEMO_FRAME_IX],\n",
    "                      image_size=DEFAULT_IMAGE_SIZE,\n",
    "                      points_per_pixel=POINTS_PER_PIXEL,\n",
    "                      radius=POINT_RADIUS)\n",
    "\n",
    "ypred_target, npred_target = get_normals_pymeshlab(ypred_target.cpu().numpy(), k=20, smoothiter=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Initial cloud (non-optimized)\")\n",
    "plt.imshow(ypred_init_target)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Optimised point cloud\")\n",
    "plt.imshow(ypred_target.detach().cpu().numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"GT mesh\")\n",
    "plt.imshow(ygts_train_target[DEMO_FRAME_IX][:, :, 0:3].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52367,
     "status": "ok",
     "timestamp": 1711046783920,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "R7Pajwlgr2UM",
    "outputId": "cd6e8bf2-5648-4855-9d58-dbb7aedb0925"
   },
   "outputs": [],
   "source": [
    "## Render the optimised cloud\n",
    "\n",
    "ypred_test = render_points(v0_norm_test.to(DEVICE),\n",
    "                      normals_to_rgb(n0_test.to(DEVICE)),\n",
    "                      elev=elevs_train_test[DEMO_FRAME_IX],\n",
    "                      azim=azims_train_test[DEMO_FRAME_IX],\n",
    "                      image_size=DEFAULT_IMAGE_SIZE,\n",
    "                      points_per_pixel=POINTS_PER_PIXEL,\n",
    "                      radius=POINT_RADIUS)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Initial cloud (non-optimized)\")\n",
    "plt.imshow(ypred_init_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Optimised point cloud\")\n",
    "plt.imshow(ypred_test.detach().cpu().numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"GT mesh\")\n",
    "plt.imshow(ygts_train_test[DEMO_FRAME_IX][:, :, 0:3].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52367,
     "status": "ok",
     "timestamp": 1711046783920,
     "user": {
      "displayName": "martin drzewiecki",
      "userId": "08830911176424240320"
     },
     "user_tz": -60
    },
    "id": "R7Pajwlgr2UM",
    "outputId": "cd6e8bf2-5648-4855-9d58-dbb7aedb0925"
   },
   "outputs": [],
   "source": [
    "# Save the optimised cloud, evaluate\n",
    "#pcu.save_mesh_vn('pcd_source.ply', v0_source.detach().cpu().numpy(), n0_source.detach().cpu().numpy())\n",
    "\n",
    "pcu.save_mesh_vn(save_source, vpred__source.detach().cpu().numpy(),npred__source.detach().cpu().numpy())\n",
    "\n",
    "#pcu.save_mesh_vn('pcd_target.ply', v0_norm_target.detach().cpu().numpy(), n0_target.detach().cpu().numpy())\n",
    "pcu.save_mesh_vn(save_target, vpred__target.detach().cpu().numpy(),nred__target.detach().cpu().numpy())\n",
    "\n",
    "#pcu.save_mesh_vn('pcd_target.ply', v0_norm_target.detach().cpu().numpy(), n0_target.detach().cpu().numpy())\n",
    "pcu.save_mesh_vn(save_test, vpred__test.detach().cpu().numpy(),npred__test.detach().cpu().numpy())\n",
    "\n",
    "print(\"-----------------------END--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually find points for the \"forced\" correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ground_points(point_cloud):\n",
    "    #z_threshold=0.00001\n",
    "    point_num=30\n",
    "    \n",
    "    point_cloud = point_cloud.cpu().detach().numpy()\n",
    "\n",
    "    # Find indices of points with lowest z values\n",
    "    indices_z = np.argsort(point_cloud[:, 2])[:1000]\n",
    "\n",
    "    # Find indices of points with lowest x values among the lowest z values\n",
    "    z_points = point_cloud[indices_z]\n",
    "    indices_xz = indices_z[np.argsort(z_points[:, 0])[:1000]]\n",
    "\n",
    "    # Find indices of points with lowest values among the lowest zx values\n",
    "    xz_points = point_cloud[indices_xz]\n",
    "    indices_xyz = indices_xz[np.argsort(xz_points[:, 1])[:point_num]]\n",
    "    print((f\"xyz values id: {indices_xyz}\"))\n",
    "    print(\"Points with xyz values:\")\n",
    "    for idx in indices_xyz:\n",
    "        print(point_cloud[idx])\n",
    "        \n",
    "    # Find indices of points with highest y values among the lowest zx values\n",
    "    indices_xYz = indices_xz[np.argsort(-xz_points[:, 1])[:point_num]]\n",
    "    print((f\"xYz values id: {indices_xYz}\"))\n",
    "    print(\"Points with xYz values:\")\n",
    "    for idx in indices_xYz:\n",
    "        print(point_cloud[idx])\n",
    "\n",
    "    # Find indices of points with highestx values among the lowest z values\n",
    "    indices_Xz = indices_z[np.argsort(-z_points[:, 0])[:1000]]\n",
    "\n",
    "    # Find indices of points with lowest values among the lowest zx values\n",
    "    Xz_points = point_cloud[indices_Xz]\n",
    "    indices_Xyz = indices_Xz[np.argsort(Xz_points[:, 1])[:point_num]]\n",
    "    print((f\"Xyz values id: {indices_Xyz}\"))\n",
    "    print(\"Points with Xyz values:\")\n",
    "    for idx in indices_Xyz:\n",
    "        print(point_cloud[idx])\n",
    "        \n",
    "    # Find indices of points with highest y values among the lowest zx values\n",
    "    indices_XYz = indices_Xz[np.argsort(-Xz_points[:, 1])[:point_num]]\n",
    "    print((f\"XYz values id: {indices_XYz}\"))\n",
    "    print(\"Points with XYz values:\")\n",
    "    for idx in indices_XYz:\n",
    "        print(point_cloud[idx])\n",
    "    \n",
    "    return \n",
    "\n",
    "def find_surface_points(point_cloud):\n",
    "    point_num=40\n",
    "\n",
    "    point_cloud = point_cloud.cpu().detach().numpy()\n",
    "\n",
    "    # Find indices of points with highest z values\n",
    "    indices_Z = np.argsort(-point_cloud[:, 2])[:1000]\n",
    "\n",
    "    # Find indices of points with lowest x values among the highest z values\n",
    "    Z_points = point_cloud[indices_Z]\n",
    "    indices_xZ = indices_Z[np.argsort(Z_points[:, 0])[:1000]]\n",
    "\n",
    "    # Find indices of points with lowest y values among the lowest x & highest z values\n",
    "    xZ_points = point_cloud[indices_xZ]\n",
    "    indices_xyZ = indices_xZ[np.argsort(xZ_points[:, 1])[:point_num]]\n",
    "    print((f\"xyZ values id: {indices_xyZ}\"))\n",
    "    print(\"Points with xyZ values:\")\n",
    "    for idx in indices_xyZ:\n",
    "        print(point_cloud[idx])\n",
    "        \n",
    "    # Find indices of points with highest y values among the lowest x & highest z values\n",
    "    indices_xYZ = indices_xZ[np.argsort(-xZ_points[:, 1])[:point_num]]\n",
    "    print((f\"xYz values id: {indices_xYZ}\"))\n",
    "    print(\"Points with xYz values:\")\n",
    "    for idx in indices_xYZ:\n",
    "        print(point_cloud[idx])\n",
    "\n",
    "    # Find indices of points with highest x values among the highestz values\n",
    "    indices_XZ = indices_Z[np.argsort(-Z_points[:, 0])[:1000]]\n",
    "\n",
    "    # Find indices of points with lowest values among the lowest zx values\n",
    "    XZ_points = point_cloud[indices_XZ]\n",
    "    indices_XyZ = indices_XZ[np.argsort(XZ_points[:, 1])[:point_num]]\n",
    "    print((f\"XyZ values id: {indices_XyZ}\"))\n",
    "    print(\"Points with Xyz values:\")\n",
    "    for idx in indices_XyZ:\n",
    "        print(point_cloud[idx])\n",
    "        \n",
    "    # Find indices of points with highest y values among the lowest zx values\n",
    "    indices_XYZ = indices_XZ[np.argsort(-XZ_points[:, 1])[:point_num]]\n",
    "    print((f\"XYZ values id: {indices_XYZ}\"))\n",
    "    print(\"Points with XYZ values:\")\n",
    "    for idx in indices_XYZ:\n",
    "        print(point_cloud[idx])\n",
    "    \n",
    "    return \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#Nearest neighbor of these two pointcloud keypoints\n",
    "def match_keypoints(src_kpoints, trg_kpoints):\n",
    "    \n",
    "    neighbors = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(trg_kpoints)\n",
    "    distances, indices = neighbors.kneighbors(src_kpoints)\n",
    "    return indices.flatten()\n",
    "\n",
    "\n",
    "\n",
    "def normalize_verts_color(vertices: torch.Tensor, scale=None, center=None) -> tuple:\n",
    "    \"\"\"\n",
    "    Normalize vertex positions of a mesh.\n",
    "\n",
    "    Args:\n",
    "        vertices (torch.Tensor): Vertex positions.\n",
    "        scale (float, optional): Scaling factor. Defaults to None.\n",
    "        center (torch.Tensor, optional): Center of the mesh. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Normalized vertex positions with corresponding \"position colors\", scaling factor, and center.\n",
    "    \"\"\"\n",
    "\n",
    "    # if center andscale known then centers the vertices around the origin and scales them\n",
    "    if scale is not None and center is not None:\n",
    "        vertices = vertices - center\n",
    "        vertices *= scale\n",
    "    else:\n",
    "        #calculate max and min of mesh vertices\n",
    "        v_max, _ = torch.max(vertices, dim=0)\n",
    "        v_min, _ = torch.min(vertices, dim=0)\n",
    "        #calculate center of mesh\n",
    "        center = (v_max + v_min) / 2.\n",
    "        #centering around origin\n",
    "        vertices = vertices - center\n",
    "        #calc max distance from vertex to origin (euklidean)\n",
    "        max_dist = torch.sqrt(torch.max(torch.sum(vertices**2, dim=-1)))\n",
    "        #scaling the max dist vertex to 1\n",
    "        scale = (1. / max_dist)\n",
    "        vertices *= scale\n",
    "\n",
    "        #now take the coordinates to change these values into color, depending on the position of the scaled version\n",
    "        colors = torch.zeros_like(vertices)\n",
    "        \n",
    "        colors[:, 0] = vertices[:, 0]  # red for x\n",
    "        colors[:, 1] = vertices[:, 1]  # green for y\n",
    "        colors[:, 2] = vertices[:, 2]  #blue for z\n",
    "\n",
    "        # normalization of color like for the vertices\n",
    "        #calculate max and min of color\n",
    "        max_values = colors.max(dim=0)[0]\n",
    "        min_values = colors.min(dim=0)[0]\n",
    "        \n",
    "        # Normalize the color channels like above\n",
    "        normalized_colors = (colors - min_values) / (max_values - min_values)\n",
    "\n",
    "        #add the color tensor to vertex tensor\n",
    "        vertices_with_colors = torch.cat((vertices, normalized_colors), dim=1)\n",
    "\n",
    "        #return vertices with color , the scale and the center of the mesh\n",
    "    return vertices_with_colors, scale, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### with keypoints ###########################################################################\n",
    "import open3d as o3d\n",
    "\n",
    "def highlight_points(points,indices):\n",
    "\n",
    "    points=points.cpu().detach().numpy()\n",
    "    #indices=indices.long().cpu().numpy()\n",
    "    indices = np.array(indices)\n",
    "    geometries = o3d.geometry.TriangleMesh()\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    \n",
    "    #spheres = []\n",
    "    geometries = [pcd]\n",
    "    for index in indices:\n",
    "        #print(indices)\n",
    "        #print(index)\n",
    "        point_coord = points[index]\n",
    "        \n",
    "        print(points[index])\n",
    "        #point_coord = np.reshape(point_coord, (3,))\n",
    "        sphere = o3d.geometry.TriangleMesh.create_sphere(radius=0.01, resolution=20) #create a small sphere to represent point\n",
    "        #sphere.translate(np.array([point_coord]))\n",
    "        sphere.translate(point_coord) #translate this sphere to point\n",
    "\n",
    "        #point_coord = np.reshape(point_coord, (3,))\n",
    "        #sphere.translate(point_coord.reshape(-1))\n",
    "\n",
    "        sphere.paint_uniform_color([0.81, 0.43, 0.79])  # Color the sphere\n",
    "        geometries.append(sphere)\n",
    "        #geometries.append(sphere)\n",
    "    #geometries.paint_uniform_color([1.0, 0.0, 0.0])\n",
    "    return geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file='pcd_table0031.ply'\n",
    "\n",
    "mesh_gt_test = torch.Tensor(pcu.load_mesh_v(test_file))\n",
    "\n",
    "vtest_norm_colors,_,_=normalize_verts_color(torch.tensor(mesh_gt_test).to(DEVICE))\n",
    "vtest_norm_colors=vtest_norm_colors.detach().clone()\n",
    "vtest_pos = vtest_norm_colors[:, :3]\n",
    "vtest_col = vtest_norm_colors[:, 3:]\n",
    "\n",
    "find_ground_points(vtest_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##################################################################################################\")\n",
    "find_surface_points(vtest_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for table20\n",
    "ground_points=[263290,133196,268633,243997 ]\n",
    "# for table 25\n",
    "#ground_points=[69152,174507,268407,41659]\n",
    "# ground points for table 31\n",
    "ground_points=[168458,36783,257424,172839]\n",
    "\n",
    "highlighted_points=highlight_points(vtest_pos,ground_points)\n",
    "o3d.visualization.draw_geometries(highlighted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for table 20\n",
    "#surface_points=[77546,12920,118464,27233]\n",
    "#surfacepoints for table25\n",
    "#surface_points=[46472,257155,298507,205242]\n",
    "#surfacepoints for table31\n",
    "surface_points=[254413,169476,158686,38907]\n",
    "highlighted_points=highlight_points(vtest_pos,surface_points)\n",
    "o3d.visualization.draw_geometries(highlighted_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/sergeyprokudin/dpf/blob/main/colab_notebooks/Static_Surface_Reconstruction_with_Optimised_Clouds.ipynb",
     "timestamp": 1710703219406
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
